# Development and Runtime Environment Options <br/>

## Description <br/>
In this module, you’ll explore how Spark processes the requests that your application submits and learn how you can track work using the Spark Application UI. Because Spark application work happens on the cluster, you need to be able to identify Apache Cluster Managers, their components, and benefits. You’ll also know how to connect with each cluster manager and how and when you might want to set up a local, standalone Spark instance. Next, you’ll learn about Apache Spark application submission, including the use of Spark’s unified interface, “spark-submit,” and learn about options and dependencies. You’ll also describe and apply options for submitting applications, identify external application dependency management techniques, and list Spark Shell benefits. You’ll also look at recommended practices for Spark's static and dynamic configuration options and perform hands-on labs to use Apache Spark on IBM Cloud and run Spark on Kubernetes. <br/>

## Objectives <br/>
* Describe Apache Spark architecture and explain Spark Application processes including Jobs, Tasks, Stages, and cluster deploy modes.
* Explain how to connect to each Apache Spark cluster mode and how and when to set up a local, standalone Spark instance.
* Describe Apache Spark application submission, including the use of Spark’s unified interface, “spark-submit.”
* Create a Python script that contains an Apache Spark job and submit the Apache Spark job to a cluster.
* List the benefits of using Apache Spark with IBM Cloud and define AIOps.
* Describe how to use Spark within AIOps and with IBM Spectrum Conductor, IBM Watson, and the IBM Analytics Engine.
* Create a Jupyter Notebook on Watson Studio that has a Python kernel with Apache Spark installed.
* Describe Apache Spark application configuration types, purposes, and common options, and describe when to use static or dynamic configurations.
* Describe when and why to use Kubernetes, how to run Kubernetes locally or on the cloud, and how to run Apache Spark on Kubernetes.
* Create a Kubernetes Pod and submit an Apache Spark job to it.