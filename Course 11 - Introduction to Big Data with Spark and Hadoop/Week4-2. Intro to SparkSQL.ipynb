{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map: applying a function to each element in the dataset\n",
    "sc = SparkContext(\"local\", \"MapExample\")\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "mapped_rdd = rdd.map(lambda x: x*2)\n",
    "mapped_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter: selecting elements based on a specific condition\n",
    "sc = SparkContext(\"local\", \"FilterExample\")\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "mapped_rdd = rdd.filter(lambda x: x % 2 == 0)\n",
    "mapped_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union: combining two datasets with the same schema\n",
    "sc = SparkContext(\"local\", \"UnionExample\")\n",
    "rdd1 = sc.parallelize([1, 2, 3])\n",
    "rdd2 = sc.parallelize([4, 5, 6])\n",
    "union_rdd = rdd1.union(rdd2)\n",
    "union_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupBy: Aggregating data based on a specific key\n",
    "sc = SparkContext(\"local\", \"GoupByExample\")\n",
    "data = [(\"apple\", 2), (\"banana\", 3), (\"apple\", 5), (\"banana\", 1)]\n",
    "rdd = sc.parallelize(data)\n",
    "grouped_data = rdd.groupBy(lambda x: x[0])\n",
    "grouped_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join: Combining two datasets based on a common key\n",
    "sc = SparkContext(\"local\", \"JoinExample\")\n",
    "rdd1 = sc.parallelize((\"apple\", 2), (\"banana\", 3))\n",
    "rdd2 = sc.parallelize((\"apple\", 5), (\"banana\", 1))\n",
    "joined_rdd = rdd1.join(rdd2)\n",
    "joined_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort: rearranging data based on a specific criterion\n",
    "sc = SparkContext(\"local\", \"SortExample\")\n",
    "data = [4, 2, 1, 3, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "sorted_rdd = rdd.sortBy(lambda x: x, ascending=True)\n",
    "sorted_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark DF: rule-based common transformations\n",
    "# 1. Predicate pushdown: Pushing filtering conditions closer to the data source before processing to minimize data movement.\n",
    "# 2. Constant folding: Evaluating constant expressions during query compilation to reduce computation during runtime.\n",
    "# 3. Column pruning: Eliminating unnecessary columns from the query plan to enhance processing efficiency.\n",
    "# 4. Join reordering: Rearranging join operations to minimize the intermediate data size and enhance the join performance.\n",
    "\n",
    "# Create a spark session\n",
    "spark = SparkSession.builder.appName(\"RuleBasedTransformations\").getOrCreate()\n",
    "\n",
    "# Sample input data for df1 and df2\n",
    "data1 = [\n",
    "    (\"Alice\", 25, \"F\"),\n",
    "    (\"Bob\", 30, \"M\"),\n",
    "    (\"Charlie\", 22, \"M\"),\n",
    "    (\"Diana\", 28, \"F\")]\n",
    "data2 = [\n",
    "    (\"Alice\", \"New York\"),\n",
    "    (\"Bob\", \"San Francisco\"),\n",
    "    (\"Charlie\", \"Los Angeles\"),\n",
    "    (\"Eve\", \"Chicago\")]\n",
    "\n",
    "# Create dfs\n",
    "columns1 = [\"name\", \"age\", \"gender\"]\n",
    "df1 = spark.createDataFrame(data1, columns1)\n",
    "columns2 = [\"name\", \"city\"]\n",
    "df2 = spark.createDataFrame(data2, columns2)\n",
    "\n",
    "# Applying predicate pushdown (filtering)\n",
    "filtered_df = df1.filter(col(\"age\") > 25)\n",
    "\n",
    "# Applying constant folding\n",
    "folded_df = filtered_df.select(col(\"name\"), col(\"age\") + 2)\n",
    "\n",
    "# Applying column pruning\n",
    "pruned_df = folded_df.select(col(\"name\"))\n",
    "\n",
    "# Join reordering\n",
    "reordered_join = df1.join(df2, on=\"name\")\n",
    "\n",
    "# Show the final results\n",
    "print(\"Filtered df:\")\n",
    "filtered_df.show()\n",
    "\n",
    "print(\"Folded df:\")\n",
    "folded_df.show()\n",
    "\n",
    "print(\"Pruned df:\")\n",
    "pruned_df.show()\n",
    "\n",
    "print(\"Reordered join df:\")\n",
    "reordered_join.show()\n",
    "\n",
    "# Stop the spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost-based optimization techniques in Spark\n",
    "# 1. Adaptive query execution: Dynamically adjusts the query plan during execution based on runtime statistics to optimize performance.\n",
    "# 2. Cost-based join reordering: Optimizes join order based on estimated costs of different join paths.\n",
    "# 3. Broadcast hash join: Optimizes small-table joins by broadcasting one table to all nodes, reducing data shuffling.\n",
    "# 4. Shuffle partitioning and memory management: Efficiently manages data shuffling during operations like groupBy and aggregation and optimizes memory usage.\n",
    "\n",
    "# Create a spark session\n",
    "spark = SparkSession.builder.appName(\"CostBasedOptimization\").getOrCreate()\n",
    "\n",
    "# Sample input data for df1 and df2 \n",
    "data1 = [\n",
    "    (\"Alice\", 25),\n",
    "    (\"Bob\", 30),\n",
    "    (\"Charlie\", 22),\n",
    "    (\"Diana\", 28)]\n",
    "data2 = [\n",
    "    (\"Alice\", \"New York\"),\n",
    "    (\"Bob\", \"San Francisco\"),\n",
    "    (\"Charlie\", \"Los Angeles\"),\n",
    "    (\"Eve\", \"Chicago\")]\n",
    "\n",
    "# Create dfs\n",
    "columns1 = [\"name\", \"age\"]\n",
    "df1 = spark.createDataFrame(df1, columns1)\n",
    "columns2 = [\"name\", \"city\"]\n",
    "df2 = spark.createDataFrame(df2, columns2)\n",
    "\n",
    "# Enable adaptive query session\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "\n",
    "# Applying adaptive query session execution (runtime adaptive optimization)\n",
    "optimized_join = df1.join(df2, on=\"name\")\n",
    "\n",
    "# Show the optimized join result\n",
    "print(\"Optimized join df:\")\n",
    "optimized_join.show()\n",
    "\n",
    "# Stop the spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
