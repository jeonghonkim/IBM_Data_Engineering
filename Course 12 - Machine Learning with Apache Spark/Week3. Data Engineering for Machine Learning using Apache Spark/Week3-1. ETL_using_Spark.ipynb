{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMSkillsNetworkBD0231ENCoursera2789-2023-01-01\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL using Spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **30** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color: red'>The purpose of this lab is to show you how to use Spark for ETL jobs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "  <li>\n",
    "    <a href=\"#Objectives\">Objectives\n",
    "    </a>\n",
    "  </li>\n",
    "  <li>\n",
    "    <a href=\"#Datasets\">Datasets\n",
    "    </a>\n",
    "  </li>\n",
    "  <li>\n",
    "    <a href=\"#Setup\">Setup\n",
    "    </a>\n",
    "    <ol>\n",
    "      <li>\n",
    "        <a href=\"#Installing-Required-Libraries\">Installing Required Libraries\n",
    "        </a>\n",
    "      </li>\n",
    "      <li>\n",
    "        <a href=\"#Importing-Required-Libraries\">Importing Required Libraries\n",
    "        </a>\n",
    "      </li>\n",
    "    </ol>\n",
    "  </li>\n",
    "  <li> \n",
    "    <a href=\"#Examples\">Examples\n",
    "    </a>\n",
    "    <ol>\n",
    "    <li>\n",
    "      <a href=\"#Task-1---Create-a-Dataframe-from-the-raw-data-and-write-to-CSV-file.\">Task 1 - Create a Dataframe from the raw data and write to CSV file.\n",
    "      </a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#Task-2---Read-from-a-csv-file-and-write-to-parquet-file\">Task 2 - Read from a csv file and write to parquet file\n",
    "      </a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#Task-3---Condense-PARQUET-to-a-single-file.\">Task 3 - Condense PARQUET to a single file.\n",
    "      </a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#Task-4---Read-from-a-parquet-file-and-write-to-csv-file\">Task 4 - Read from a parquet file and write to csv file\n",
    "      </a>\n",
    "    </li>\n",
    "      </ol>\n",
    "  <li>\n",
    "    <a href=\"#Exercises\">Exercises\n",
    "    </a>\n",
    "  </li>\n",
    "  <ol>\n",
    "    <li>\n",
    "      <a href=\"#Exercise-1---Extract\">Exercise 1 - Extract\n",
    "      </a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#Exercise-2---Transform\">Exercise 2 - Transform\n",
    "      </a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#Exercise-3---Load\">Exercise 3 - Load\n",
    "      </a>\n",
    "    </li>\n",
    "  </ol>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    " \n",
    " - Create a Spark Dataframe from the raw data and write to CSV file.\n",
    " - Read from a csv file and write to parquet file\n",
    " - Condense PARQUET to a single file.\n",
    " - Read from a parquet file and write to csv file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we will be using the following libraries:\n",
    "\n",
    "*   [`PySpark`](https://spark.apache.org/docs/latest/api/python/index.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMSkillsNetworkBD0231ENCoursera2789-2023-01-01) for connecting to the Spark Cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Required Libraries\n",
    "\n",
    "Spark Cluster is pre-installed in the Skills Network Labs environment. However, you need libraries like pyspark and findspark to connect to this cluster.\n",
    "\n",
    "If you wish to download this jupyter notebook and run on your local computer, follow the instructions mentioned <a href=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-BD0231EN-Coursera/labs/Connecting_to_spark_cluster_using_Skills_Network_labs.ipynb\">here.</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pyspark==3.1.2 -q\n",
    "!pip install findspark -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "\n",
    "_We recommend you import all required libraries in one place (here):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# FindSpark simplifies the process of using Apache Spark with Python\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/28 21:38:52 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "#Create SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ETL using Spark\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Create a Dataframe from the raw data and write to CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create a list of tuples\n",
    "#each tuple contains the student id, height and weight\n",
    "data = [(\"student1\",64,90),\n",
    "        (\"student2\",59,100),\n",
    "        (\"student3\",69,95),\n",
    "        (\"\",70,110),\n",
    "        (\"student5\",60,80),\n",
    "        (\"student3\",69,95),\n",
    "        (\"student6\",62,85),\n",
    "        (\"student7\",65,80),\n",
    "        (\"student7\",65,80)]\n",
    "\n",
    "# some rows are intentionally duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create a dataframe using createDataFrame and pass the data and the column names.\n",
    "df = spark.createDataFrame(data, [\"student\", \"height_inches\", \"weight_pounds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------+\n",
      "| student|height_inches|weight_pounds|\n",
      "+--------+-------------+-------------+\n",
      "|student1|           64|           90|\n",
      "|student2|           59|          100|\n",
      "|student3|           69|           95|\n",
      "|        |           70|          110|\n",
      "|student5|           60|           80|\n",
      "+--------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the data frame\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.mode(\"overwrite\").csv(\"student-hw.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you do not wish to over write use df.write.csv(\"student-hw.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------+\n",
      "| student|height_inches|weight_pounds|\n",
      "+--------+-------------+-------------+\n",
      "|student7|           65|           80|\n",
      "|student7|           65|           80|\n",
      "|student2|           59|          100|\n",
      "|student1|           64|           90|\n",
      "|student3|           69|           95|\n",
      "+--------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load student dataset\n",
    "df = spark.read.csv(\"student-hw.csv\", header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Read from a csv file and write to parquet file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------+\n",
      "| student|height_inches|weight_pounds|\n",
      "+--------+-------------+-------------+\n",
      "|student7|           65|           80|\n",
      "|student7|           65|           80|\n",
      "|student2|           59|          100|\n",
      "|student1|           64|           90|\n",
      "|student3|           69|           95|\n",
      "|student5|           60|           80|\n",
      "|student3|           69|           95|\n",
      "|student6|           62|           85|\n",
      "|    null|           70|          110|\n",
      "+--------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load student dataset\n",
    "df = spark.read.csv(\"student-hw.csv\", header=True, inferSchema=True)\n",
    "# display dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the number of rows in the dataframe\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------+\n",
      "| student|height_inches|weight_pounds|\n",
      "+--------+-------------+-------------+\n",
      "|student6|           62|           85|\n",
      "|student3|           69|           95|\n",
      "|student2|           59|          100|\n",
      "|student7|           65|           80|\n",
      "|    null|           70|          110|\n",
      "|student1|           64|           90|\n",
      "|student5|           60|           80|\n",
      "+--------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.dropDuplicates()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice that the duplicates are removed\n",
    "# print the number of rows in the dataframe\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Null values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------+\n",
      "| student|height_inches|weight_pounds|\n",
      "+--------+-------------+-------------+\n",
      "|student6|           62|           85|\n",
      "|student3|           69|           95|\n",
      "|student2|           59|          100|\n",
      "|student7|           65|           80|\n",
      "|student1|           64|           90|\n",
      "|student5|           60|           80|\n",
      "+--------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Observe the rows with null values getting dropped\n",
    "df = df.dropna()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to parquet file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write the data to a Parquet file\n",
    "# if you do not wish to overwrite use the command df.write.parquet(\"student-hw.parquet\")\n",
    "df.write.mode(\"overwrite\").parquet(\"student-hw.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['total 7',\n",
       " '-rw-r--r-- 1 jupyterlab resources   0 Jan 28 21:44 _SUCCESS',\n",
       " '-rw-r--r-- 1 jupyterlab resources 467 Jan 28 21:44 part-00000-cb3ed2cb-1b85-4838-a3de-ac7d1aba04ff-c000.snappy.parquet',\n",
       " '-rw-r--r-- 1 jupyterlab resources 911 Jan 28 21:44 part-00003-cb3ed2cb-1b85-4838-a3de-ac7d1aba04ff-c000.snappy.parquet',\n",
       " '-rw-r--r-- 1 jupyterlab resources 911 Jan 28 21:44 part-00010-cb3ed2cb-1b85-4838-a3de-ac7d1aba04ff-c000.snappy.parquet',\n",
       " '-rw-r--r-- 1 jupyterlab resources 911 Jan 28 21:44 part-00054-cb3ed2cb-1b85-4838-a3de-ac7d1aba04ff-c000.snappy.parquet',\n",
       " '-rw-r--r-- 1 jupyterlab resources 911 Jan 28 21:44 part-00132-cb3ed2cb-1b85-4838-a3de-ac7d1aba04ff-c000.snappy.parquet',\n",
       " '-rw-r--r-- 1 jupyterlab resources 911 Jan 28 21:44 part-00172-cb3ed2cb-1b85-4838-a3de-ac7d1aba04ff-c000.snappy.parquet',\n",
       " '-rw-r--r-- 1 jupyterlab resources 911 Jan 28 21:44 part-00186-cb3ed2cb-1b85-4838-a3de-ac7d1aba04ff-c000.snappy.parquet']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify that the parquet file(s) are created\n",
    "!!ls -l student-hw.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are a lot of .parquet files in the output.\n",
    "- To improve parallellism, spark stores each dataframe in multiple partitions.\n",
    "- When the data is saved as parquet file, each partition is saved as a separate file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Condense PARQUET to a single file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce the number of partitions in the dataframe to one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.repartition(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to parquet file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write the data to a Parquet file\n",
    "# If you do not wish to overwrite use the command df.write.parquet(\"student-hw-single.parquet\")\n",
    "df.write.mode(\"overwrite\").parquet(\"student-hw-single.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2\n",
      "-rw-r--r-- 1 jupyterlab resources   0 Jan 28 21:45 _SUCCESS\n",
      "-rw-r--r-- 1 jupyterlab resources 944 Jan 28 21:45 part-00000-b1286f7b-e0c7-44c4-b2e3-27d5dd704e73-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "# Verify that the parquet file(s) are created\n",
    "# Notice that there is only one .parquet file\n",
    "!ls -l student-hw-single.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - Read from a parquet file and write to csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------+\n",
      "| student|height_inches|weight_pounds|\n",
      "+--------+-------------+-------------+\n",
      "|student6|           62|           85|\n",
      "|student3|           69|           95|\n",
      "|student2|           59|          100|\n",
      "|student7|           65|           80|\n",
      "|student1|           64|           90|\n",
      "|student5|           60|           80|\n",
      "+--------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"student-hw-single.parquet\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import the expr function that helps in transforming the data\n",
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert inches to centimeters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------+------------------+\n",
      "| student|height_inches|weight_pounds|height_centimeters|\n",
      "+--------+-------------+-------------+------------------+\n",
      "|student6|           62|           85|            157.48|\n",
      "|student3|           69|           95|            175.26|\n",
      "|student2|           59|          100|            149.86|\n",
      "|student7|           65|           80|            165.10|\n",
      "|student1|           64|           90|            162.56|\n",
      "|student5|           60|           80|            152.40|\n",
      "+--------+-------------+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert inches to centimeters\n",
    "# Multiply the column height_inches with 2.54 to get a new column height_centimeters\n",
    "df = df.withColumn(\"height_centimeters\", expr(\"height_inches * 2.54\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert pounds to kilograms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------+------------------+---------+\n",
      "| student|height_inches|weight_pounds|height_centimeters|weight_kg|\n",
      "+--------+-------------+-------------+------------------+---------+\n",
      "|student6|           62|           85|            157.48|38.555320|\n",
      "|student3|           69|           95|            175.26|43.091240|\n",
      "|student2|           59|          100|            149.86|45.359200|\n",
      "|student7|           65|           80|            165.10|36.287360|\n",
      "|student1|           64|           90|            162.56|40.823280|\n",
      "|student5|           60|           80|            152.40|36.287360|\n",
      "+--------+-------------+-------------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert pounds to kilograms\n",
    "# Multiply weight_pounds with 0.453592 to get a new column weight_kg\n",
    "df = df.withColumn(\"weight_kg\", expr(\"weight_pounds * 0.453592\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+------------------+---------+\n",
      "| student|height_inches|height_centimeters|weight_kg|\n",
      "+--------+-------------+------------------+---------+\n",
      "|student6|           62|            157.48|38.555320|\n",
      "|student3|           69|            175.26|43.091240|\n",
      "|student2|           59|            149.86|45.359200|\n",
      "|student7|           65|            165.10|36.287360|\n",
      "|student1|           64|            162.56|40.823280|\n",
      "|student5|           60|            152.40|36.287360|\n",
      "+--------+-------------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop the columns \"height_inches\",\"weight_pounds\"\n",
    "df = df.drop(\"heihgt_inches\", \"weight_pounds\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename a column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+---------+---------+\n",
      "| student|height_inches|height_cm|weight_kg|\n",
      "+--------+-------------+---------+---------+\n",
      "|student6|           62|   157.48|38.555320|\n",
      "|student3|           69|   175.26|43.091240|\n",
      "|student2|           59|   149.86|45.359200|\n",
      "|student7|           65|   165.10|36.287360|\n",
      "|student1|           64|   162.56|40.823280|\n",
      "|student5|           60|   152.40|36.287360|\n",
      "+--------+-------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rename the lengthy column name \"height_centimeters\" to \"height_cm\"\n",
    "df = df.withColumnRenamed(\"height_centimeters\", \"height_cm\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.mode(\"overwrite\").csv(\"student_transformed.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+---------+---------+\n",
      "| student|height_inches|height_cm|weight_kg|\n",
      "+--------+-------------+---------+---------+\n",
      "|student6|           62|   157.48| 38.55532|\n",
      "|student3|           69|   175.26| 43.09124|\n",
      "|student2|           59|   149.86|  45.3592|\n",
      "|student7|           65|    165.1| 36.28736|\n",
      "|student1|           64|   162.56| 40.82328|\n",
      "|student5|           60|    152.4| 36.28736|\n",
      "+--------+-------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load student dataset\n",
    "df = spark.read.csv(\"student_transformed.csv\", header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Exercise - ETL using Spark\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Extract\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from student_transformed.csv into a dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+---------+---------+\n",
      "| student|height_inches|height_cm|weight_kg|\n",
      "+--------+-------------+---------+---------+\n",
      "|student6|           62|   157.48| 38.55532|\n",
      "|student3|           69|   175.26| 43.09124|\n",
      "|student2|           59|   149.86|  45.3592|\n",
      "|student7|           65|    165.1| 36.28736|\n",
      "|student1|           64|   162.56| 40.82328|\n",
      "+--------+-------------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load student dataset\n",
    "df = spark.read.csv(\"student_transformed.csv\", header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - Transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert cm to meters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import the expr function that helps in transforming the data\n",
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+---------+---------+------------------+\n",
      "| student|height_inches|height_cm|weight_kg|     height_meters|\n",
      "+--------+-------------+---------+---------+------------------+\n",
      "|student6|           62|   157.48| 38.55532|            1.5748|\n",
      "|student3|           69|   175.26| 43.09124|            1.7526|\n",
      "|student2|           59|   149.86|  45.3592|1.4986000000000002|\n",
      "|student7|           65|    165.1| 36.28736|             1.651|\n",
      "|student1|           64|   162.56| 40.82328|            1.6256|\n",
      "|student5|           60|    152.4| 36.28736|             1.524|\n",
      "+--------+-------------+---------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert centimeters to meters\n",
    "# Divide the column height_cm by 100 a new column height_meters\n",
    "df = df.withColumn(\"height_meters\", expr(\"height_cm / 100\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a column named bmi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+---------+---------+------------------+------------------+\n",
      "| student|height_inches|height_cm|weight_kg|     height_meters|               BMI|\n",
      "+--------+-------------+---------+---------+------------------+------------------+\n",
      "|student6|           62|   157.48| 38.55532|            1.5748|15.546531093062187|\n",
      "|student3|           69|   175.26| 43.09124|            1.7526|14.028892161964118|\n",
      "|student2|           59|   149.86|  45.3592|1.4986000000000002|20.197328530250278|\n",
      "|student7|           65|    165.1| 36.28736|             1.651|13.312549228648752|\n",
      "|student1|           64|   162.56| 40.82328|            1.6256|15.448293591899683|\n",
      "|student5|           60|    152.4| 36.28736|             1.524|15.623755691955827|\n",
      "+--------+-------------+---------+---------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compute bmi using the below formula\n",
    "# BMI = weight/(height * height)\n",
    "# weight must be in kgs\n",
    "# height must be in meters\n",
    "df = df.withColumn(\"BMI\", expr(\"weight_kg / (height_meters * height_meters)\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the columns height_inches, weight_pounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+------------------+------------------+\n",
      "| student|height_cm|weight_kg|     height_meters|               BMI|\n",
      "+--------+---------+---------+------------------+------------------+\n",
      "|student6|   157.48| 38.55532|            1.5748|15.546531093062187|\n",
      "|student3|   175.26| 43.09124|            1.7526|14.028892161964118|\n",
      "|student2|   149.86|  45.3592|1.4986000000000002|20.197328530250278|\n",
      "|student7|    165.1| 36.28736|             1.651|13.312549228648752|\n",
      "|student1|   162.56| 40.82328|            1.6256|15.448293591899683|\n",
      "|student5|    152.4| 36.28736|             1.524|15.623755691955827|\n",
      "+--------+---------+---------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop the columns \"height_inches\",\"weight_pounds\"\n",
    "df = df.drop(\"height_inches\", \"weight_pounds\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+------------------+------------------+-----------+\n",
      "| student|height_cm|weight_kg|     height_meters|               BMI|BMI_rounded|\n",
      "+--------+---------+---------+------------------+------------------+-----------+\n",
      "|student6|   157.48| 38.55532|            1.5748|15.546531093062187|       16.0|\n",
      "|student3|   175.26| 43.09124|            1.7526|14.028892161964118|       14.0|\n",
      "|student2|   149.86|  45.3592|1.4986000000000002|20.197328530250278|       20.0|\n",
      "|student7|    165.1| 36.28736|             1.651|13.312549228648752|       13.0|\n",
      "|student1|   162.56| 40.82328|            1.6256|15.448293591899683|       15.0|\n",
      "|student5|    152.4| 36.28736|             1.524|15.623755691955827|       16.0|\n",
      "+--------+---------+---------+------------------+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Round the column bmi\n",
    "from pyspark.sql.functions import col, round\n",
    "df = df.withColumn(\"BMI_rounded\", round(col(\"BMI\")))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 - Load\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the dataframe into a parquet file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Write the data to a Parquet file, set the mode to overwrite\n",
    "df.write.mode(\"overwrite\").parquet(\"studnet_transformed.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations you have completed this lab.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ramesh Sannareddy](https://www.linkedin.com/in/rsannareddy/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMBD0231ENSkillsNetwork866-2023-01-01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2023-05-21|0.1|Ramesh Sannareddy|Initial Version Created|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2023 IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
